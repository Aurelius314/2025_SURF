'''

è¿™æ˜¯ä¸€ä¸ªç”± FastAPI æ­å»ºçš„ç¤ºä¾‹åç«¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯è¿è¡Œ `uvicorn main:app --reload` å¯åŠ¨å¹¶è®¿é—®ã€‚

'''

import torch
from fastapi import FastAPI, HTTPException, Body
from PIL import Image
import cn_clip.clip as clip
import io, os, pickle, base64
from lmdb import open as open_lmdb
from tqdm import tqdm
from pathlib import Path
import sys  
sys.path.append(r'E:/surf')  
from utils import load_surf_checkpoint_model_from_base, load_text_data_from_lmdb, load_images_from_paths  
from cache_manager import cache_image_features, cache_text_features


# -------åˆå§‹åŒ–-----------
app = FastAPI()
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = load_surf_checkpoint_model_from_base(
    ckpt_path="E:/SURF2025/æ£€ç´¢æ–‡åŒ–æ•°æ®é›†/checkpoint/epoch_latest.pt"
)
model.eval()

CACHE_DIR = "E:/surf/cache"
LMDB_PATH = "E:/surf/valid_output_lmdb"    # dataset!!!
FEATURES_CACHE = os.path.join(CACHE_DIR, "image_features.pt")

# å•å¼ å£ç”»imageï¼Œç”¨äºæµ‹è¯•
TEST_IMAGE_PATH = r"E:\surf\test examples\image1.png"
TEST_IMAGE_FEATURE_CACHE = r"E:\surf\cached_test_img\test_image_features.pt"


# API Key æ¨¡æ‹Ÿ
API_KEYS = {"demo": "your_demo_api_key"}

IMAGE_ROOT = "E:/SURF2025/æ£€ç´¢æ–‡åŒ–æ•°æ®é›†/images/images"  # æ”¹æˆä½ æœ¬åœ°å›¾åƒæ ¹ç›®å½•

def verify_api_key(api_key: str):
    if api_key not in API_KEYS.values():
        raise HTTPException(status_code=403, detail="Unauthorized")
    
def fix_base64_padding(b64_string: str) -> str:
    return b64_string + '=' * ((4 - len(b64_string) % 4) % 4)

@app.get("/")
def read_root():
    return {"message": "Welcome to the CN-CLIP API"}

# -------------åŠ è½½æ•°æ®------------------------
# ä»lmdbä¸­å–å‡ºæ¥ï¼Œéƒ½æ˜¯list
# suffix: ['images/1000.png']
text_ids, original_texts, nld_texts, image_ids_list, img_rel_path, splits = load_text_data_from_lmdb(LMDB_PATH)

# imageæœ¬èº«
# image_paths_list: ä¸¤éƒ¨åˆ†æ‹¼èµ·æ¥ ---â†’ æ‹¿åˆ°å›¾åƒè·¯å¾„list
image_list, image_paths_list = load_images_from_paths(img_rel_path)

# å…¨å±€recordï¼Œå¯å¤ç”¨
data_records = [
    {
        "image": image_paths_list[i],
        "image_id": image_ids_list[i],
        "text_id": text_ids[i],
        "original_text": original_texts[i],
        "NLD_text": nld_texts[i],
        "image_path": image_paths_list[i][0] if image_paths_list[i] else ""
    }
    for i in range(len(image_paths_list))
]

# ç‰¹å¾å‘é‡
image_features_tensor = cache_image_features(model, preprocess, data_records, CACHE_DIR)


# ç¼“å­˜æ–‡æœ¬ç‰¹å¾ï¼ˆåŸºäº original_textsï¼Œå› ä¸ºè¾ƒçŸ­ï¼Œç¼“å­˜è¾ƒå°ï¼ŒåŠ è½½æ›´å¿«ï¼‰
text_features_tensor = cache_text_features(model, data_records, CACHE_DIR)
# print(type(text_features_tensor)) <class 'torch.Tensor'>

# --------åŠ è½½ç¼“å­˜  é—®é¢˜æ¥äº†ï¼šç¼“å­˜äº†å“ªäº›ä¸œè¥¿ï¼Ÿ
if os.path.exists(FEATURES_CACHE):
    print("Loading cached image features...")
    image_features_tensor = torch.load(FEATURES_CACHE).to(device)
    print(f"Loaded {len(image_features_tensor)} image features.")  # Loaded 577(576+1) image features. 

# -------æˆ–é‡æ–°æå–
else:
    print("Loading LMDB records and image filesï¼Ÿ...")


    # æå–å›¾åƒè·¯å¾„åˆ—è¡¨
    env = open_lmdb(LMDB_PATH, readonly=True, lock=False)
    image_paths_list = []
    with env.begin() as txn:
        for key, value in txn.cursor():
            data = pickle.loads(value)
            image_paths_list.append(data["image_paths"])
    env.close()


    print("Calculating image features...")
    image_features_list = []


    for i, img in enumerate(tqdm(image_paths_list, desc="ğŸ” Extracting image features")):
        if img is None:
            image_features_list.append(torch.zeros(model.visual.output_dim))  # å ä½å‘é‡
            continue

        # å¦‚æœå½“å‰å›¾åƒæ˜¯æµ‹è¯•å›¾åƒï¼Œåˆ™ç”¨ç¼“å­˜ç‰¹å¾
        if Path(TEST_IMAGE_PATH).as_posix():
            img_filename = os.path.basename(TEST_IMAGE_PATH)
            print(f"[ç¼“å­˜å‘½ä¸­] ä½¿ç”¨ {img_filename} çš„ç¼“å­˜ç‰¹å¾")
            feat = torch.load(TEST_IMAGE_FEATURE_CACHE)
            image_features_list.append(feat.cpu())     # æŠŠæˆ‘ä»¬è‡ªå·±çš„æµ‹è¯•å›¾åƒç‰¹å¾ä¹ŸåŠ è¿›å»äº†
            continue

        # æ­£å¸¸æå–ç‰¹å¾
        img_tensor = preprocess(img).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = model.encode_image(img_tensor)
            feat /= feat.norm(dim=-1, keepdim=True) + 1e-7
            image_features_list.append(feat.cpu())

    # æ‹¼æ¥ä¸ºä¸€ä¸ª tensor
    image_features_tensor = torch.stack(image_features_list, dim=0)

    # ç¼“å­˜ä¿å­˜
    os.makedirs(CACHE_DIR, exist_ok=True)
    torch.save(image_features_tensor, FEATURES_CACHE)
    print(f"Saved {len(image_ids_list)} features to cache.")


# -------------------- å›¾æœæ–‡ --------------------

@app.post("/image-to-text/")
async def image_to_text(img_base64: str = Body(...), api_key: str = Body(...)):
    try:
        verify_api_key(api_key)

        # æ„å»º"text"çš„ç»“æ„åŒ–è®°å½•
        text_records = [
            {
                "text_id": tid,
                "original_text": ori,
                "NLD_text": nld,
            }
            for tid, ori, nld in zip(text_ids, original_texts, nld_texts)
        ]

        # å»é‡åŸæ–‡,seenå·²ç»è§è¿‡çš„
        # record_mappingæœ‰ç”¨å—
        seen = set()
        text_candidates, record_mapping = [], []
        for i, rec in enumerate(text_records):
            if rec["original_text"] not in seen:
                seen.add(rec["original_text"])
                text_candidates.append(rec["original_text"])
                record_mapping.append(i)

        print("ä½¿ç”¨çš„ text_candidates æ•°é‡:", len(text_candidates))



        with torch.no_grad():
            # å·²ç¼“å­˜å¥½çš„ç‰¹å¾å¼ é‡
            image_features = image_features_tensor / image_features_tensor.norm(dim=1, keepdim=True)
            text_features = text_features_tensor / text_features_tensor.norm(dim=1, keepdim=True)

            logit_scale = model.logit_scale.exp()
            logits_per_image = logit_scale * image_features @ text_features.t()
            probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]



        # Top-K ç»“æœ
        topk_indices = probs.argsort()[-9:][::-1]
        results = []
        for i, idx in enumerate(topk_indices):
            # å›¾æœæ–‡ï¼Œå…ˆä¸è¦è¾“å‡ºæ–‡æœ¬å¯¹åº”çš„å›¾ç‰‡
            rec = text_records[record_mapping[idx]]

            results.append({
                "rank": i + 1,
                "text_id": rec["text_id"],
                "original_text": rec["original_text"],
                "NLD_text": rec["NLD_text"],
                # "image_path": image_path_abs,
                # "image_base64": db_image_base64,
                "score": round(probs[idx] * 100, 3)
            })

        return {"top_k_results": results}


    except Exception as e:
        import traceback
        print("ERROR:", traceback.format_exc())
        return {"error": str(e)}


# -------------------- æ–‡æœå›¾ --------------------

@app.post("/text-to-image/")
async def text_to_image(query_text: str = Body(...), api_key: str = Body(...)):
    verify_api_key(api_key)

    with torch.no_grad():
        text_features = text_features_tensor / text_features_tensor.norm(dim=1, keepdim=True)
        image_features = image_features_tensor / image_features_tensor.norm(dim=1, keepdim=True)

        logit_scale = model.logit_scale.exp()
        logits_per_text = logit_scale * text_features @ image_features.t()
        probs = logits_per_text.softmax(dim=-1).cpu().numpy()[0]

    topk_indices = probs.argsort()[-9:][::-1]
    results = []
    for i, idx in enumerate(topk_indices):
        img = image_list[idx]
        buffered = io.BytesIO()
        img.save(buffered, format="JPEG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")

        results.append({
            "rank": i + 1,
            "image_id": image_ids_list[idx],
            "text_id": text_ids[idx],
            "original_text": original_texts[idx],
            "NLD_text": nld_texts[idx],
            "image_path": image_paths_list[idx] if idx < len(image_paths_list) else None,
            "image_base64": img_base64,
            "score": round(probs[idx] * 100, 3)
        })

        print(f"image_paths_list: {image_ids_list[idx]}")
        print(f"text_list: {original_texts[idx]}")
        print(f"nld_list: {nld_texts[idx]}")
        print(f"idx: {idx}")


    return {"top_k_results": results}


